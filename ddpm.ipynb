{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/telejkoi/anaconda3/envs/dl2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel, AutoencoderKL\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/data0/lsun/bedroom'\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    \n",
    "    vae_url = \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors\"\n",
    "\n",
    "    def __init__(self, device):\n",
    "        self.model = AutoencoderKL.from_single_file(self.vae_url).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def to_latent(self, input):\n",
    "        with torch.no_grad():\n",
    "            latent = self.model.encode(input.to(self.device))\n",
    "        return latent.latent_dist.sample()\n",
    "\n",
    "    def to_image(self, encoded):\n",
    "        with torch.no_grad():\n",
    "            output_img = self.model.decode(encoded)\n",
    "        return output_img.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/telejkoi/anaconda3/envs/dl2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: vae.to_latent(x.unsqueeze(0)).squeeze(0))  \n",
    "])\n",
    "\n",
    "image_dataset = ImageFolder(root=DATA_PATH, transform=transform)\n",
    "image_dataset = Subset(image_dataset, torch.randperm(len(image_dataset))[:1000])\n",
    "train_dataloader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for idx, batch in enumerate(train_dataloader):\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(n, d_model):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    assert d_model % 2 == 0, 'd_model must be divisible by 2'\n",
    "    wk = torch.tensor([1 / 10_000 ** (2 * i / d_model) for i in range(d_model // 2)])\n",
    "    wk = wk.reshape((1, d_model // 2))\n",
    "    t = torch.arange(n).reshape((n, 1))\n",
    "    encoding = torch.zeros(n, d_model)\n",
    "    encoding[:,::2] = torch.sin(t * wk)\n",
    "    encoding[:, 1::2] = torch.cos(t * wk)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    \"\"\"Embedding block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, n_steps, d_model):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "        self.t_embed = self.init_pos_encoding(n_steps, 10)\n",
    "        self.l1 = nn.Linear(10, 20)\n",
    "        self.l2 = nn.Linear(20, d_model)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def init_pos_encoding(self, n_steps, d_model):\n",
    "        t_embed = nn.Embedding(n_steps, d_model)\n",
    "        t_embed.weight.data = pos_encoding(n_steps, d_model)\n",
    "        t_embed.requires_grad = False\n",
    "        return t_embed\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.t_embed(t)\n",
    "        t = self.l1(t)\n",
    "        t = self.silu(t)\n",
    "        t = self.l2(t)\n",
    "        return t\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Encoder block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        pool = self.pool(x)\n",
    "        return x, pool\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Decoder block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = ConvBlock(2*out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, down_tensor):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat((x, down_tensor), dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet model for diffusion.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, n_steps, input_size=32, in_channels=4, first_layer_channels=64):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # input size\n",
    "        self.s1 = input_size\n",
    "        self.s2 = self.s1 // 2\n",
    "        self.s3 = self.s2 // 2\n",
    "\n",
    "        # number of channels\n",
    "        self.ch0 = in_channels\n",
    "        self.ch1 = first_layer_channels\n",
    "        self.ch2 = self.ch1 * 2\n",
    "        self.ch3 = self.ch2 * 2\n",
    "\n",
    "        # embedding blocks\n",
    "        self.em1 = EmbeddingBlock(n_steps, in_channels * self.s1 * self.s1)\n",
    "        self.em2 = EmbeddingBlock(n_steps, self.ch1 * self.s2 * self.s2)\n",
    "        self.em3 = EmbeddingBlock(n_steps, self.ch2 * self.s3 * self.s3)\n",
    "        self.em4 = EmbeddingBlock(n_steps, self.ch3 * self.s3 * self.s3)\n",
    "        self.em5 = EmbeddingBlock(n_steps, self.ch2 * self.s2 * self.s2)\n",
    "\n",
    "        # encoder blocks\n",
    "        self.e1 = EncoderBlock(self.ch0, self.ch1)\n",
    "        self.e2 = EncoderBlock(self.ch1, self.ch2)\n",
    "\n",
    "        # decoder blocks\n",
    "        self.d1 = DecoderBlock(self.ch3, self.ch2)\n",
    "        self.d2 = DecoderBlock(self.ch2, self.ch1)\n",
    "\n",
    "        # middle conv block\n",
    "        self.middle = ConvBlock(self.ch2, self.ch3)\n",
    "\n",
    "        # output layer\n",
    "        self.out = nn.Conv2d(self.ch1, self.ch0, kernel_size=1, padding=\"same\")\n",
    "\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t1 = self.em1(t).view(-1, self.ch0, self.s1, self.s1)\n",
    "        t2 = self.em2(t).view(-1, self.ch1, self.s2, self.s2)\n",
    "        t3 = self.em3(t).view(-1, self.ch2, self.s3, self.s3)\n",
    "        t4 = self.em4(t).view(-1, self.ch3, self.s3, self.s3)\n",
    "        t5 = self.em5(t).view(-1, self.ch2, self.s2, self.s2)\n",
    "\n",
    "        x1, pool1 = self.e1(x + t1)\n",
    "        x2, pool2 = self.e2(pool1 + t2)\n",
    "        x = self.middle(pool2 + t3)\n",
    "        x = self.d1(x + t4, x2)\n",
    "        x = self.d2(x + t5, x1)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# model = UNet(BATCH_SIZE, 1000)\n",
    "# summary(model, input_size=[(BATCH_SIZE, 4, 32, 32), (BATCH_SIZE,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 8, 8]) torch.Size([8, 256, 8, 8])\n",
      "torch.Size([8, 256, 16, 16])\n",
      "torch.Size([8, 128, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 32, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = UNet(BATCH_SIZE, 100).to(device)\n",
    "unet(batch[0], torch.arange(BATCH_SIZE).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
