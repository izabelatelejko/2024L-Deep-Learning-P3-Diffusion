{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/telejkoi/anaconda3/envs/dl2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel, AutoencoderKL\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/data0/lsun/bedroom'\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    vae_url = \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors\"\n",
    "\n",
    "    def __init__(self, device):\n",
    "        self.model = AutoencoderKL.from_single_file(self.vae_url).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def to_latent(self, input):\n",
    "        with torch.no_grad():\n",
    "            latent = self.model.encode(input.to(self.device))\n",
    "        return latent.latent_dist.sample()\n",
    "\n",
    "    def to_image(self, encoded):\n",
    "        with torch.no_grad():\n",
    "            output_img = self.model.decode(encoded)\n",
    "        return output_img.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/telejkoi/anaconda3/envs/dl2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: vae.to_latent(x.unsqueeze(0)).squeeze(0))  \n",
    "])\n",
    "\n",
    "image_dataset = ImageFolder(root=DATA_PATH, transform=transform)\n",
    "image_dataset = Subset(image_dataset, torch.randperm(len(image_dataset))[:1000])\n",
    "train_dataloader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for idx, batch in enumerate(train_dataloader):\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_encoding(n, d_model):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    assert d_model % 2 == 0, 'd_model must be divisible by 2'\n",
    "    wk = torch.tensor([1 / 10_000 ** (2 * i / d_model) for i in range(d_model // 2)])\n",
    "    wk = wk.reshape((1, d_model // 2))\n",
    "    t = torch.arange(n).reshape((n, 1))\n",
    "    encoding = torch.zeros(n, d_model)\n",
    "    encoding[:,::2] = torch.sin(t * wk)\n",
    "    encoding[:, 1::2] = torch.cos(t * wk)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    \"\"\"Embedding block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, n_steps, d_model):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "        self.t_embed = self.init_pos_encoding(n_steps, d_model)\n",
    "        self.l1 = nn.Linear(d_model, d_model)\n",
    "        self.l2 = nn.Linear(d_model, d_model)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def init_pos_encoding(self, n_steps, d_model):\n",
    "        t_embed = nn.Embedding(n_steps, d_model)\n",
    "        t_embed.weight.data = pos_encoding(n_steps, d_model)\n",
    "        t_embed.requires_grad = False\n",
    "        return t_embed\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.t_embed(t)\n",
    "        t = self.l1(t)\n",
    "        t = self.silu(t)\n",
    "        t = self.l2(t)\n",
    "        return t\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Encoder block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        pool = self.pool(x)\n",
    "        return x, pool\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Decoder block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = ConvBlock(2*out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, down_tensor):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat((x, down_tensor), dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet model for diffusion.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, n_steps, input_size=32, in_channels=4, first_layer_channels=64):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # input size\n",
    "        self.s1 = input_size\n",
    "        self.s2 = self.s1 // 2\n",
    "        self.s3 = self.s2 // 2\n",
    "\n",
    "        # number of channels\n",
    "        self.ch0 = in_channels\n",
    "        self.ch1 = first_layer_channels\n",
    "        self.ch2 = self.ch1 * 2\n",
    "        self.ch3 = self.ch2 * 2\n",
    "\n",
    "        # positional encoding\n",
    "        # self.t1 = self.init_pos_encoding(n_steps, d_model = in_channels * self.s1 * self.s1)\n",
    "        # self.t2 = self.init_pos_encoding(n_steps, d_model = self.ch1 * self.s2 * self.s2)\n",
    "        # self.t3 = self.init_pos_encoding(n_steps, d_model = self.ch2 * self.s3 * self.s3)\n",
    "\n",
    "        # embedding blocks\n",
    "        self.em1 = EmbeddingBlock(n_steps, in_channels * self.s1 * self.s1)\n",
    "        self.em2 = EmbeddingBlock(n_steps, self.ch1 * self.s2 * self.s2)\n",
    "        self.em3 = EmbeddingBlock(n_steps, self.ch2 * self.s3 * self.s3)\n",
    "        self.em4 = EmbeddingBlock(n_steps, self.ch2 * self.s2 * self.s2)\n",
    "        self.em5 = EmbeddingBlock(n_steps, self.ch1 * self.s1 * self.s1)\n",
    "\n",
    "        # encoder blocks\n",
    "        self.e1 = EncoderBlock(self.ch0, self.ch1)\n",
    "        self.e2 = EncoderBlock(self.ch1, self.ch2)\n",
    "\n",
    "        # decoder blocks\n",
    "        self.d1 = DecoderBlock(self.ch3, self.ch2)\n",
    "        self.d2 = DecoderBlock(self.ch2, self.ch1)\n",
    "\n",
    "        # middle conv block\n",
    "        self.middle = ConvBlock(self.ch2, self.ch3)\n",
    "\n",
    "        # output layer\n",
    "        self.out = nn.Conv2d(self.ch1, self.ch0, kernel_size=1, padding=\"same\")\n",
    "\n",
    "\n",
    "    # def init_pos_encoding(self, n_steps, d_model):\n",
    "    #     t_embed = nn.Embedding(n_steps, d_model)\n",
    "    #     t_embed.weight.data = pos_encoding(n_steps, d_model)\n",
    "    #     t_embed.requires_grad = False\n",
    "    #     return t_embed\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        t1 = self.em1(t).view(-1, self.ch0, self.s1, self.s1)\n",
    "        t2 = self.em2(t).view(-1, self.ch1, self.s2, self.s2)\n",
    "        t3 = self.em3(t).view(-1, self.ch2, self.s3, self.s3)\n",
    "        t4 = self.em4(t).view(-1, self.ch2, self.s2, self.s2)\n",
    "        t5 = self.em5(t).view(-1, self.ch1, self.s1, self.s1)\n",
    "\n",
    "        x1, pool1 = self.e1(x + t1)\n",
    "        x2, pool2 = self.e2(pool1 + t2)\n",
    "\n",
    "        x = self.middle(pool2 + t3)\n",
    "\n",
    "        x = self.d1(x + t4, x2)\n",
    "        x = self.d2(x + t5, x1)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# model = UNet(BATCH_SIZE, 1000)\n",
    "# summary(model, input_size=[(BATCH_SIZE, 4, 32, 32), (BATCH_SIZE,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 17179869184 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unet \u001b[38;5;241m=\u001b[39m \u001b[43mUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m unet(batch[\u001b[38;5;241m0\u001b[39m], torch\u001b[38;5;241m.\u001b[39marange(BATCH_SIZE)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "Cell \u001b[0;32mIn[7], line 100\u001b[0m, in \u001b[0;36mUNet.__init__\u001b[0;34m(self, batch_size, n_steps, input_size, in_channels, first_layer_channels)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem3 \u001b[38;5;241m=\u001b[39m EmbeddingBlock(n_steps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem4 \u001b[38;5;241m=\u001b[39m EmbeddingBlock(n_steps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms2)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem5 \u001b[38;5;241m=\u001b[39m \u001b[43mEmbeddingBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mch1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# encoder blocks\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me1 \u001b[38;5;241m=\u001b[39m EncoderBlock(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch1)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mEmbeddingBlock.__init__\u001b[0;34m(self, n_steps, d_model)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28msuper\u001b[39m(EmbeddingBlock, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_pos_encoding(n_steps, d_model)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, d_model)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msilu \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSiLU()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2/lib/python3.10/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 17179869184 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "unet = UNet(BATCH_SIZE, 100).to(device)\n",
    "unet(batch[0], torch.arange(BATCH_SIZE).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
