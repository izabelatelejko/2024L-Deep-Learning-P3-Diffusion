{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/telejkoi/anaconda3/envs/dl2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.models.vae import VAE\n",
    "from src.const import DATA_PATH, SEED\n",
    "from src.preprocess import get_data_loader\n",
    "from src.models.utils import pos_encoding\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/telejkoi/anaconda3/envs/dl2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(device)\n",
    "train_dataloader = get_data_loader(DATA_PATH, BATCH_SIZE, vae)\n",
    "\n",
    "for idx, batch in enumerate(train_dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    \"\"\"Embedding block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, n_steps, d_model):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.t_embed = self.init_pos_encoding(d_model)\n",
    "        # self.l1 = nn.Linear(16, 32)\n",
    "        # self.l2 = nn.Linear(32, d_model)\n",
    "        # self.silu = nn.SiLU()\n",
    "\n",
    "    def init_pos_encoding(self, d_model):\n",
    "        t_embed = nn.Embedding(self.n_steps, d_model)\n",
    "        t_embed.weight.data = pos_encoding(self.n_steps, d_model)\n",
    "        t_embed.requires_grad = False\n",
    "        return t_embed\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.t_embed(t)\n",
    "        # t = self.l1(t)\n",
    "        # t = self.silu(t)\n",
    "        # t = self.l2(t)\n",
    "        return t\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    \"\"\"Downsample block block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownsampleBlock, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        pool = self.pool(x)\n",
    "        return x, pool\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"Upsample block for UNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = ConvBlock(2*out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, down_tensor):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat((x, down_tensor), dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet model for diffusion.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, n_steps, input_size=32, in_channels=4, first_layer_channels=64):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # input size\n",
    "        self.s1 = input_size\n",
    "        self.s2 = self.s1 // 2\n",
    "        self.s3 = self.s2 // 2\n",
    "\n",
    "        # number of channels\n",
    "        self.ch0 = in_channels\n",
    "        self.ch1 = first_layer_channels\n",
    "        self.ch2 = self.ch1 * 2\n",
    "        self.ch3 = self.ch2 * 2\n",
    "\n",
    "        # embedding blocks\n",
    "        self.em1 = EmbeddingBlock(n_steps, in_channels * self.s1 * self.s1)\n",
    "        self.em2 = EmbeddingBlock(n_steps, self.ch1 * self.s2 * self.s2)\n",
    "        self.em3 = EmbeddingBlock(n_steps, self.ch2 * self.s3 * self.s3)\n",
    "        self.em4 = EmbeddingBlock(n_steps, self.ch3 * self.s3 * self.s3)\n",
    "        self.em5 = EmbeddingBlock(n_steps, self.ch2 * self.s2 * self.s2)\n",
    "\n",
    "        # downsample blocks\n",
    "        self.e1 = DownsampleBlock(self.ch0, self.ch1)\n",
    "        self.e2 = DownsampleBlock(self.ch1, self.ch2)\n",
    "\n",
    "        # upsample blocks\n",
    "        self.d1 = UpsampleBlock(self.ch3, self.ch2)\n",
    "        self.d2 = UpsampleBlock(self.ch2, self.ch1)\n",
    "\n",
    "        # middle conv block\n",
    "        self.middle = ConvBlock(self.ch2, self.ch3)\n",
    "\n",
    "        # output layer\n",
    "        self.out = nn.Conv2d(self.ch1, self.ch0, kernel_size=1, padding=\"same\")\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t1 = self.em1(t).view(-1, self.ch0, self.s1, self.s1)\n",
    "        t2 = self.em2(t).view(-1, self.ch1, self.s2, self.s2)\n",
    "        t3 = self.em3(t).view(-1, self.ch2, self.s3, self.s3)\n",
    "        t4 = self.em4(t).view(-1, self.ch3, self.s3, self.s3)\n",
    "        t5 = self.em5(t).view(-1, self.ch2, self.s2, self.s2)\n",
    "\n",
    "        x1, pool1 = self.e1(x + t1)\n",
    "        x2, pool2 = self.e2(pool1 + t2)\n",
    "        x = self.middle(pool2 + t3)\n",
    "        x = self.d1(x + t4, x2)\n",
    "        x = self.d2(x + t5, x1)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineScheduler():\n",
    "\n",
    "    def __init__(self, n_steps, device):\n",
    "        # Save the device\n",
    "        self.device = device\n",
    "        t_vals = torch.arange(0, n_steps, 1).to(torch.int).to(self.device)\n",
    "\n",
    "        def f(t):\n",
    "            s = 0.008\n",
    "            return torch.clamp(torch.cos(((t/n_steps + s)/(1+s)) * (torch.pi/2))**2 /\\\n",
    "                torch.cos(torch.tensor((s/(1+s)) * (torch.pi/2)))**2,\n",
    "                1e-10,\n",
    "                0.999)\n",
    "\n",
    "        # alpha_bar_t is defined directly from the scheduler\n",
    "        self.a_bar_t = f(t_vals+1).to(self.device)\n",
    "        self.a_bar_t1 = f((t_vals).clamp(0, torch.inf)).to(self.device)\n",
    "\n",
    "        # beta_t and alpha_t are defined from a_bar_t\n",
    "        self.beta_t = 1 - (self.a_bar_t / self.a_bar_t1)\n",
    "        self.beta_t = torch.clamp(self.beta_t, 1e-10, 0.999).to(self.device)\n",
    "        self.a_t = 1 - self.beta_t\n",
    "\n",
    "        # Roots of a and a_bar\n",
    "        self.sqrt_a_t = torch.sqrt(self.a_t).to(self.device)\n",
    "        self.sqrt_a_bar_t = torch.sqrt(self.a_bar_t).to(self.device)\n",
    "        self.sqrt_1_minus_a_bar_t = torch.sqrt(1-self.a_bar_t).to(self.device)\n",
    "        self.sqrt_a_bar_t1 = torch.sqrt(self.a_bar_t1).to(self.device)\n",
    "\n",
    "        # Beta tilde value\n",
    "        self.beta_tilde_t = (((1 - self.a_bar_t1)/(1 - self.a_bar_t)) * self.beta_t).to(self.device)\n",
    "\n",
    "        self.beta_t = self.beta_t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.a_t = self.a_t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.a_bar_t = self.a_bar_t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.a_bar_t1 = self.a_bar_t1.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.sqrt_a_t = self.sqrt_a_t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.sqrt_a_bar_t = self.sqrt_a_bar_t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.sqrt_1_minus_a_bar_t = self.sqrt_1_minus_a_bar_t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.sqrt_a_bar_t1 = self.sqrt_a_bar_t1.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.beta_tilde_t = self.beta_tilde_t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDDPM(nn.Module):\n",
    "    \"\"\"Improved DDPM model for diffusion.\"\"\"\n",
    "\n",
    "    def __init__(self, unet, device):\n",
    "        super(IDDPM, self).__init__()\n",
    "        self.unet = unet.to(device)\n",
    "        self.device = device\n",
    "        self.n_steps = unet.em1.n_steps\n",
    "        self.scheduler = CosineScheduler(self.n_steps, self.device)\n",
    "        self.out_mean = nn.Conv2d(4, 4, 3, padding=1, groups=4)\n",
    "        self.out_var = nn.Conv2d(4, 4, 3, padding=1, groups=4)\n",
    "\n",
    "    def forward(self, x, t, eps=None):\n",
    "        if eps is None:\n",
    "            eps = torch.randn(x.shape).to(self.device)\n",
    "        x_with_noise = self.scheduler.sqrt_a_bar_t[t] * x + self.scheduler.sqrt_1_minus_a_bar_t[t] * eps\n",
    "        return x_with_noise\n",
    "    \n",
    "    def backward(self, x, t):\n",
    "        print(x.shape, t.shape)\n",
    "        out = self.unet(x, t)\n",
    "        noise, v = out[:, :4], out[:, :4]\n",
    "        noise = self.out_mean(noise)\n",
    "        v = self.out_var(v)\n",
    "        return noise, v\n",
    "    \n",
    "    def noise_to_mean(self, epsilon, x_t, t, corrected=True):\n",
    "        # Note: Corrected function from the following:\n",
    "        # https://github.com/hojonathanho/diffusion/issues/5\n",
    "        beta_t = self.scheduler.beta_t[t]\n",
    "        sqrt_a_t = self.scheduler.sqrt_a_t[t]\n",
    "        a_bar_t = self.scheduler.a_bar_t[t]\n",
    "        sqrt_a_bar_t = self.scheduler.sqrt_a_bar_t[t]\n",
    "        sqrt_1_minus_a_bar_t = self.scheduler.sqrt_1_minus_a_bar_t[t]\n",
    "        a_bar_t1 = self.scheduler.a_bar_t1[t]\n",
    "        sqrt_a_bar_t1 = self.scheduler.sqrt_a_bar_t1[t]\n",
    "\n",
    "        mean = torch.where(t == 0,\n",
    "            (1 / sqrt_a_t) * (x_t - (beta_t / sqrt_1_minus_a_bar_t) * epsilon),\n",
    "            (sqrt_a_bar_t1 * beta_t) / (1 - a_bar_t) * \\\n",
    "                torch.clamp((1 / sqrt_a_bar_t) * x_t - (sqrt_1_minus_a_bar_t / sqrt_a_bar_t) * epsilon, -1, 1 ) + \\\n",
    "                (((1 - a_bar_t1) * sqrt_a_t) / (1 - a_bar_t)) * x_t\n",
    "        )\n",
    "        return mean\n",
    "    \n",
    "    def vs_to_variance(self, v, t):\n",
    "        beta_t = self.scheduler.beta_t[t]\n",
    "        beta_tilde_t = self.scheduler.beta_tilde_t[t]\n",
    "        \n",
    "        # Return the variance value\n",
    "        return torch.exp(torch.clamp(v * torch.log(beta_t) + (1 - v) * torch.log(beta_tilde_t), torch.tensor(-30, device=self.device), torch.tensor(30, device=self.device)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IDDPMTrainer:\n",
    "    def __init__(self, iddpm, device, lr=1e-4, Lambda=1e-3):\n",
    "        self.n_steps = iddpm.n_steps\n",
    "        self.Lambda = Lambda\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.device = device\n",
    "        self.model = iddpm\n",
    "        self.t_vals = np.arange(0, self.n_steps, 1)\n",
    "        self.t_dist = torch.distributions.uniform.Uniform(\n",
    "            float(1) - float(0.499), float(self.n_steps) + float(0.499)\n",
    "        )\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr, eps=1e-4)\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"none\").to(self.device)\n",
    "        self.losses = np.zeros((self.n_steps, 10))\n",
    "        self.losses_ct = np.zeros(self.n_steps, dtype=int)\n",
    "\n",
    "    def update_losses(self, loss_vlb, t):\n",
    "        for t_val, loss in zip(t, loss_vlb):\n",
    "            if self.losses_ct[t_val] == 10:\n",
    "                self.losses[t_val] = np.concatenate((self.losses[t_val][1:], [loss]))\n",
    "            else:\n",
    "                self.losses[t_val, self.losses_ct[t_val]] = loss\n",
    "                self.losses_ct[t_val] += 1\n",
    "\n",
    "    def loss_simple(self, eps, eps_theta):\n",
    "        return ((eps_theta - eps) ** 2).flatten(1, -1).mean(-1)\n",
    "\n",
    "    # Formula derived from: https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n",
    "    def loss_vlb_gauss(self, mean_real, mean_fake, var_real, var_fake):\n",
    "        \"\"\"KL divergence between two gaussians.\"\"\"\n",
    "        std_real = torch.sqrt(var_real)\n",
    "        std_fake = torch.sqrt(var_fake)\n",
    "        kl_div = (\n",
    "            (\n",
    "                torch.log(std_fake / std_real)\n",
    "                + ((var_real) + (mean_real - mean_fake) ** 2) / (2 * (var_fake))\n",
    "                - torch.tensor(1 / 2)\n",
    "            )\n",
    "            .flatten(1, -1)\n",
    "            .mean(-1)\n",
    "        )\n",
    "        return kl_div\n",
    "\n",
    "    def calc_losses(self, eps, eps_theta, var_theta, x, x_with_noise, t):\n",
    "\n",
    "        mean_t_pred = self.model.noise_to_mean(eps_theta, x_with_noise, t, True)\n",
    "        var_t_pred = self.model.vs_to_variance(var_theta, t)\n",
    "\n",
    "        beta_t = self.model.scheduler.beta_t[t]\n",
    "        a_bar_t = self.model.scheduler.a_bar_t[t]\n",
    "        a_bar_t1 = self.model.scheduler.a_bar_t1[t]\n",
    "        beta_tilde_t = self.model.scheduler.beta_tilde_t[t]\n",
    "        sqrt_a_bar_t1 = self.model.scheduler.sqrt_a_bar_t1[t]\n",
    "        sqrt_a_t = self.model.scheduler.sqrt_a_t[t]\n",
    "\n",
    "        mean_t = ((sqrt_a_bar_t1 * beta_t) / (1 - a_bar_t)) * x + (\n",
    "            (sqrt_a_t * (1 - a_bar_t1)) / (1 - a_bar_t)\n",
    "        ) * x_with_noise\n",
    "\n",
    "        loss_simple = self.loss_simple(eps, eps_theta)\n",
    "        loss_vlb = (\n",
    "            self.loss_vlb_gauss(\n",
    "                mean_t, mean_t_pred.detach(), beta_tilde_t, var_t_pred\n",
    "            )\n",
    "            * self.Lambda\n",
    "        )\n",
    "        loss_hybrid = loss_simple + loss_vlb\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t = t.detach().cpu().numpy()\n",
    "            loss = loss_vlb.detach().cpu()\n",
    "            self.update_losses(loss, t)\n",
    "\n",
    "            if np.sum(self.losses_ct) == self.losses.size - 20:\n",
    "                p_t = np.sqrt((self.losses**2).mean(-1))\n",
    "                p_t = p_t / p_t.sum()\n",
    "                loss = loss / torch.tensor(p_t[t], device=self.device)\n",
    "\n",
    "        return loss_hybrid.mean(), loss_simple.mean(), loss_vlb.mean()\n",
    "\n",
    "    def train(self, loader, n_epochs, model_store_path=None):\n",
    "        self.model.train()\n",
    "        n = len(loader.dataset)\n",
    "\n",
    "        self.losses_comb = np.array([])\n",
    "        self.losses_mean = np.array([])\n",
    "        self.losses_var = np.array([])\n",
    "        self.steps_list = np.array([])\n",
    "\n",
    "        losses_comb_s = torch.tensor(0.0, requires_grad=False)\n",
    "        losses_mean_s = torch.tensor(0.0, requires_grad=False)\n",
    "        losses_var_s = torch.tensor(0.0, requires_grad=False)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for _, batch in enumerate(\n",
    "                tqdm(loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", colour=\"#005500\")\n",
    "            ):\n",
    "                x = batch[0]\n",
    "                if np.sum(self.losses_ct) == self.losses.size - 20:\n",
    "                    # Weights for each value of t\n",
    "                    p_t = np.sqrt((self.losses**2).mean(-1))\n",
    "                    p_t = p_t / p_t.sum()\n",
    "                    t = torch.tensor(\n",
    "                        np.random.choice(self.t_vals, size=x.shape[0], p=p_t),\n",
    "                        device=self.device,\n",
    "                    )\n",
    "                else:\n",
    "                    t = self.t_dist.sample((x.shape[0],)).to(self.device)\n",
    "                    t = torch.round(t).to(torch.long)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    eps = torch.randn_like(x).to(device)\n",
    "                    x_with_noise = self.model(x, t, eps)\n",
    "\n",
    "                print(x_with_noise.shape, t.shape)\n",
    "                eps_theta, var_theta = self.model.backward(x_with_noise, t)\n",
    "                loss, loss_mean, loss_var = self.calc_losses(\n",
    "                    eps, eps_theta, var_theta, x, x_with_noise, t\n",
    "                )\n",
    "                loss = loss * x.shape[0] / n\n",
    "                loss_mean = loss_mean * x.shape[0] / n\n",
    "                loss_var = loss_var * x.shape[0] / n\n",
    "\n",
    "                # Backprop the loss, but save the intermediate gradients\n",
    "                loss.backward()\n",
    "\n",
    "                losses_comb_s += loss.cpu().detach()\n",
    "                losses_mean_s += loss_mean.cpu().detach()\n",
    "                losses_var_s += loss_var.cpu().detach()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                self.losses_comb = np.append(self.losses_comb, losses_comb_s.item())\n",
    "                self.losses_mean = np.append(self.losses_mean, losses_mean_s.item())\n",
    "                self.losses_var = np.append(self.losses_var, losses_var_s.item())\n",
    "\n",
    "                losses_comb_s *= 0\n",
    "                losses_mean_s *= 0\n",
    "                losses_var_s *= 0\n",
    "\n",
    "            print(\n",
    "                f\"Loss at epoch {epoch + 1}: \"\n",
    "                + f\"Combined: {round(self.losses_comb[-10:].mean(), 4)}    \"\n",
    "                f\"Mean: {round(self.losses_mean[-10:].mean(), 4)}    \"\n",
    "                f\"Variance: {round(self.losses_var[-10:].mean(), 6)}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            if model_store_path is not None and self.best_loss > loss:\n",
    "                self.best_loss = loss\n",
    "                torch.save(self.model.state_dict(), model_store_path)\n",
    "                print(\"Best model ever (stored)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iddpm = IDDPM(UNet(batch_size=BATCH_SIZE, n_steps=64), device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|\u001b[38;2;0;85;0m          \u001b[0m| 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 32, 32]) torch.Size([32])\n",
      "torch.Size([32, 4, 32, 32]) torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   6%|\u001b[38;2;0;85;0m▋         \u001b[0m| 2/32 [00:05<01:16,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 32, 32]) torch.Size([32])\n",
      "torch.Size([32, 4, 32, 32]) torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "Epoch 1/10:   6%|\u001b[38;2;0;85;0m▋         \u001b[0m| 2/32 [00:07<01:51,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 32, 32]) torch.Size([32])\n",
      "torch.Size([32, 4, 32, 32]) torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m IDDPMTrainer(iddpm, device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 117\u001b[0m, in \u001b[0;36mIDDPMTrainer.train\u001b[0;34m(self, loader, n_epochs, model_store_path)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_with_noise\u001b[38;5;241m.\u001b[39mshape, t\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    116\u001b[0m eps_theta, var_theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackward(x_with_noise, t)\n\u001b[0;32m--> 117\u001b[0m loss, loss_mean, loss_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_losses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_with_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m n\n\u001b[1;32m    121\u001b[0m loss_mean \u001b[38;5;241m=\u001b[39m loss_mean \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m n\n",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mIDDPMTrainer.calc_losses\u001b[0;34m(self, eps, eps_theta, var_theta, x, x_with_noise, t)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_losses\u001b[39m(\u001b[38;5;28mself\u001b[39m, eps, eps_theta, var_theta, x, x_with_noise, t):\n\u001b[1;32m     46\u001b[0m     mean_t_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnoise_to_mean(eps_theta, x_with_noise, t, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 47\u001b[0m     var_t_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvs_to_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     beta_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mbeta_t[t]\n\u001b[1;32m     50\u001b[0m     a_bar_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39ma_bar_t[t]\n",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m, in \u001b[0;36mIDDPM.vs_to_variance\u001b[0;34m(self, v, t)\u001b[0m\n\u001b[1;32m     48\u001b[0m beta_tilde_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mbeta_tilde_t[t]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Return the variance value\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mclamp(v \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(beta_t) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m v) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(beta_tilde_t), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m30\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer = IDDPMTrainer(iddpm, device)\n",
    "trainer.train(train_dataloader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
